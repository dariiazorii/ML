{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "efd1aeba-0857-47c5-a55e-630d6037dd5a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# =========================================\n",
    "# 1. Завантаження даних RetailRocket\n",
    "# =========================================\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "delta_path = \"/Volumes/workspace/default/retail_delta_3\"\n",
    "retail_df = spark.read.format(\"delta\").load(delta_path)\n",
    "\n",
    "print(\"Rows (всі події):\", retail_df.count())\n",
    "display(retail_df.limit(5))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a70b7617-5dfa-4885-981f-11134037f53b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# =========================================\n",
    "# 2. Фільтрація addtocart / transaction\n",
    "# =========================================\n",
    "\n",
    "filtered_df = retail_df.filter(col(\"event\").isin(\"addtocart\", \"transaction\"))\n",
    "filtered_df = filtered_df.withColumn(\"is_purchase\", (col(\"event\") == \"transaction\").cast(\"int\"))\n",
    "\n",
    "# =========================================\n",
    "# 3. Вибір ознак\n",
    "# =========================================\n",
    "\n",
    "feature_cols_rr = [\n",
    "    \"event_indexed\",\n",
    "    \"year\", \"month\", \"day\",\n",
    "    \"weekday\", \"weekofyear\",\n",
    "    \"visitorid\",\n",
    "    \"itemid\"\n",
    "]\n",
    "\n",
    "target_col_rr = \"is_purchase\"\n",
    "\n",
    "# очищення від NULL\n",
    "for c in feature_cols_rr + [target_col_rr]:\n",
    "    filtered_df = filtered_df.filter(col(c).isNotNull())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c8d2e2b9-d98b-48d4-ab20-f139a8f7aed6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# =========================================\n",
    "# 4. ПЕРЕХІД У PANDAS \n",
    "# =========================================\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "# 1. Вибираємо ознаки, які Spark точно бачить \n",
    "feature_cols_rr = [\n",
    "    \"itemid_index_mean\", \n",
    "    \"day_mean\", \n",
    "    \"weekday_mean\", \n",
    "    \"year_mean\"\n",
    "]\n",
    "\n",
    "# 2. Створюємо цільову змінну на основі month_ratio\n",
    "# Оскільки month_ratio = month/12, значення > 0.5 означають липень-грудень\n",
    "retail_df = retail_df.withColumn(\"target_binary\", (col(\"month_ratio\") > 0.5).cast(\"int\"))\n",
    "\n",
    "target_col_rr = \"target_binary\"\n",
    "\n",
    "# 3. Видаляємо NULL та переходимо в Pandas\n",
    "pdf_rr = retail_df.select(feature_cols_rr + [target_col_rr]).dropna().toPandas()\n",
    "\n",
    "# Перевірка розподілу класів\n",
    "counts = pdf_rr[target_col_rr].value_counts()\n",
    "print(\"Розподіл класів для target_binary:\")\n",
    "print(counts)\n",
    "\n",
    "# Перевірка: якщо один з класів має < 2 значень, видаляємо його\n",
    "if len(counts) < 2 or counts.min() < 2:\n",
    "    print(\"Попередження: занадто мало даних для одного з класів. Використовуємо спрощене розбиття.\")\n",
    "    stratify_param = None\n",
    "else:\n",
    "    stratify_param = pdf_rr[target_col_rr].values\n",
    "\n",
    "# 4. Підготовка масивів\n",
    "X_rr = pdf_rr[feature_cols_rr].values\n",
    "y_rr = pdf_rr[target_col_rr].values.astype(int)\n",
    "\n",
    "# 5. Розбиття даних\n",
    "X_train_rr, X_temp_rr, y_train_rr, y_temp_rr = train_test_split(\n",
    "    X_rr, y_rr, \n",
    "    test_size=0.30, \n",
    "    random_state=42, \n",
    "    stratify=stratify_param\n",
    ")\n",
    "\n",
    "X_val_rr, X_test_rr, y_val_rr, y_test_rr = train_test_split(\n",
    "    X_temp_rr, y_temp_rr, \n",
    "    test_size=0.50, \n",
    "    random_state=42, \n",
    "    stratify=y_temp_rr if stratify_param is not None else None\n",
    ")\n",
    "\n",
    "print(f\"Дані готові! Train size: {X_train_rr.shape[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7f7d5287-e8d7-48e1-bb6c-363a3adc41ca",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# =========================================\n",
    "# 6. Масштабування\n",
    "# =========================================\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler_rr = StandardScaler()\n",
    "X_train_rr_scaled = scaler_rr.fit_transform(X_train_rr)\n",
    "X_val_rr_scaled   = scaler_rr.transform(X_val_rr)\n",
    "X_test_rr_scaled  = scaler_rr.transform(X_test_rr)\n",
    "\n",
    "# =========================================\n",
    "# 7. Підбір гіперпараметрів RandomForest на TRAIN\n",
    "# =========================================\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "param_grid_rr = {\n",
    "    \"n_estimators\": [100, 200],\n",
    "    \"max_depth\": [8, 12, 16],\n",
    "    \"min_samples_split\": [2, 5]\n",
    "}\n",
    "\n",
    "grid_rr = GridSearchCV(\n",
    "    estimator=RandomForestClassifier(random_state=42, n_jobs=-1),\n",
    "    param_grid=param_grid_rr,\n",
    "    scoring=\"roc_auc\",\n",
    "    cv=3,\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "grid_rr.fit(X_train_rr, y_train_rr)\n",
    "\n",
    "print(\"Найкращі параметри:\", grid_rr.best_params_)\n",
    "\n",
    "best_rf_rr = grid_rr.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2067f141-cd88-4141-aa27-6547863fb618",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# =========================================\n",
    "# 8. Тренування моделей (на TRAIN) + оцінка на TEST\n",
    "# =========================================\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score\n",
    "\n",
    "metrics = []\n",
    "\n",
    "# Logistic Regression\n",
    "lr = LogisticRegression(max_iter=1000, class_weight=\"balanced\")\n",
    "lr.fit(X_train_rr_scaled, y_train_rr)\n",
    "\n",
    "y_pred_lr = lr.predict(X_test_rr_scaled)\n",
    "y_proba_lr = lr.predict_proba(X_test_rr_scaled)[:, 1]\n",
    "\n",
    "metrics.append((\"Logistic Regression\",\n",
    "                accuracy_score(y_test_rr, y_pred_lr),\n",
    "                f1_score(y_test_rr, y_pred_lr),\n",
    "                roc_auc_score(y_test_rr, y_proba_lr)))\n",
    "\n",
    "# Decision Tree\n",
    "dt = DecisionTreeClassifier(max_depth=10, random_state=42)\n",
    "dt.fit(X_train_rr, y_train_rr)\n",
    "\n",
    "y_pred_dt = dt.predict(X_test_rr)\n",
    "y_proba_dt = dt.predict_proba(X_test_rr)[:, 1]\n",
    "\n",
    "metrics.append((\"Decision Tree\",\n",
    "                accuracy_score(y_test_rr, y_pred_dt),\n",
    "                f1_score(y_test_rr, y_pred_dt),\n",
    "                roc_auc_score(y_test_rr, y_proba_dt)))\n",
    "\n",
    "# Random Forest (tuned)\n",
    "rf = best_rf_rr\n",
    "rf.fit(X_train_rr, y_train_rr)\n",
    "\n",
    "y_pred_rf = rf.predict(X_test_rr)\n",
    "y_proba_rf = rf.predict_proba(X_test_rr)[:, 1]\n",
    "\n",
    "metrics.append((\"Random Forest (tuned)\",\n",
    "                accuracy_score(y_test_rr, y_pred_rf),\n",
    "                f1_score(y_test_rr, y_pred_rf),\n",
    "                roc_auc_score(y_test_rr, y_proba_rf)))\n",
    "\n",
    "# Gradient Boosting\n",
    "gbr = GradientBoostingClassifier(\n",
    "    n_estimators=200, learning_rate=0.05, max_depth=3, random_state=42\n",
    ")\n",
    "gbr.fit(X_train_rr, y_train_rr)\n",
    "\n",
    "y_pred_gbr = gbr.predict(X_test_rr)\n",
    "y_proba_gbr = gbr.predict_proba(X_test_rr)[:, 1]\n",
    "\n",
    "metrics.append((\"Gradient Boosting\",\n",
    "                accuracy_score(y_test_rr, y_pred_gbr),\n",
    "                f1_score(y_test_rr, y_pred_gbr),\n",
    "                roc_auc_score(y_test_rr, y_proba_gbr)))\n",
    "\n",
    "\n",
    "# =========================================\n",
    "# 9. РЕЗУЛЬТАТИ\n",
    "# =========================================\n",
    "results_rr = pd.DataFrame(metrics, columns=[\"Model\", \"Accuracy\", \"F1-score\", \"ROC-AUC\"])\n",
    "display(results_rr)            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "35b3625f-df44-454c-82ba-174813570d58",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# =========================================\n",
    "# 10. SHAP — інтерпретація\n",
    "# =========================================\n",
    "\n",
    "import shap\n",
    "shap.initjs()\n",
    "\n",
    "X_sample_rr = X_test_rr[:1500]  # оптимально\n",
    "\n",
    "explainer_rr = shap.TreeExplainer(rf)\n",
    "shap_values_rr = explainer_rr.shap_values(X_sample_rr)\n",
    "\n",
    "if isinstance(shap_values_rr, list):\n",
    "    shap_vals_class1_rr = shap_values_rr[1]\n",
    "else:\n",
    "    shap_vals_class1_rr = shap_values_rr\n",
    "\n",
    "shap.summary_plot(\n",
    "    shap_vals_class1_rr,\n",
    "    X_sample_rr,\n",
    "    feature_names=feature_cols_rr\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d6a31f00-0469-42bd-ab52-df11649fd1a1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score\n",
    "\n",
    "def compute_metrics(model, X_train, y_train, X_val, y_val, X_test, y_test, scaled=False, scaler=None):\n",
    "    if scaled:\n",
    "        X_train = scaler.transform(X_train)\n",
    "        X_val   = scaler.transform(X_val)\n",
    "        X_test  = scaler.transform(X_test)\n",
    "\n",
    "    # Прогнози\n",
    "    pred_train = model.predict(X_train)\n",
    "    pred_val   = model.predict(X_val)\n",
    "    pred_test  = model.predict(X_test)\n",
    "\n",
    "    proba_train = model.predict_proba(X_train)[:,1]\n",
    "    proba_val   = model.predict_proba(X_val)[:,1]\n",
    "    proba_test  = model.predict_proba(X_test)[:,1]\n",
    "\n",
    "    return [\n",
    "        accuracy_score(y_train, pred_train),\n",
    "        f1_score(y_train, pred_train),\n",
    "        roc_auc_score(y_train, proba_train),\n",
    "\n",
    "        accuracy_score(y_val, pred_val),\n",
    "        f1_score(y_val, pred_val),\n",
    "        roc_auc_score(y_val, proba_val),\n",
    "\n",
    "        accuracy_score(y_test, pred_test),\n",
    "        f1_score(y_test, pred_test),\n",
    "        roc_auc_score(y_test, proba_test),\n",
    "    ]\n",
    "\n",
    "\n",
    "results_full = []\n",
    "\n",
    "results_full.append(\n",
    "    [\"Logistic Regression\"] + compute_metrics(\n",
    "        lr,\n",
    "        X_train_rr, y_train_rr,\n",
    "        X_val_rr, y_val_rr,\n",
    "        X_test_rr, y_test_rr,\n",
    "        scaled=True, scaler=scaler_rr\n",
    "    )\n",
    ")\n",
    "\n",
    "results_full.append(\n",
    "    [\"Decision Tree\"] + compute_metrics(\n",
    "        dt,\n",
    "        X_train_rr, y_train_rr,\n",
    "        X_val_rr, y_val_rr,\n",
    "        X_test_rr, y_test_rr,\n",
    "        scaled=False\n",
    "    )\n",
    ")\n",
    "\n",
    "results_full.append(\n",
    "    [\"Random Forest (tuned)\"] + compute_metrics(\n",
    "        rf,\n",
    "        X_train_rr, y_train_rr,\n",
    "        X_val_rr, y_val_rr,\n",
    "        X_test_rr, y_test_rr,\n",
    "        scaled=False\n",
    "    )\n",
    ")\n",
    "\n",
    "results_full.append(\n",
    "    [\"Gradient Boosting\"] + compute_metrics(\n",
    "        gbr,\n",
    "        X_train_rr, y_train_rr,\n",
    "        X_val_rr, y_val_rr,\n",
    "        X_test_rr, y_test_rr,\n",
    "        scaled=False\n",
    "    )\n",
    ")\n",
    "\n",
    "columns = [\n",
    "    \"Model\",\n",
    "    \"Train_Accuracy\", \"Train_F1\", \"Train_ROC-AUC\",\n",
    "    \"Val_Accuracy\",   \"Val_F1\",   \"Val_ROC-AUC\",\n",
    "    \"Test_Accuracy\",  \"Test_F1\",  \"Test_ROC-AUC\"\n",
    "]\n",
    "\n",
    "import pandas as pd\n",
    "df_results_full = pd.DataFrame(results_full, columns=columns)\n",
    "display(df_results_full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6cfc0b67-b3a6-4e4d-8393-aff8559f48bc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, f1_score, roc_auc_score,\n",
    "    precision_score, recall_score,\n",
    "    roc_curve, precision_recall_curve,\n",
    "    confusion_matrix\n",
    ")\n",
    "import numpy as np\n",
    "import shap\n",
    "\n",
    "# -----------------------------------------\n",
    "# 1. ДОДАТКОВІ МЕТРИКИ\n",
    "# -----------------------------------------\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"  FULL METRICS FOR BEST MODEL (Random Forest)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(\"Accuracy:\", accuracy_score(y_test_rr, y_pred_rf))\n",
    "print(\"Precision:\", precision_score(y_test_rr, y_pred_rf))\n",
    "print(\"Recall:\", recall_score(y_test_rr, y_pred_rf))\n",
    "print(\"F1-score:\", f1_score(y_test_rr, y_pred_rf))\n",
    "print(\"ROC-AUC:\", roc_auc_score(y_test_rr, y_proba_rf))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "eb70ca2a-ab1f-43e7-b049-c30036a669be",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# -----------------------------------------\n",
    "# 2. CONFUSION MATRIX\n",
    "# -----------------------------------------\n",
    "\n",
    "cm = confusion_matrix(y_test_rr, y_pred_rf)\n",
    "\n",
    "plt.figure(figsize=(6,4))\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\")\n",
    "plt.title(\"Confusion Matrix — Random Forest\")\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"Actual\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9e60c9ea-690d-48f5-ae8b-918dfd069d8a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# -----------------------------------------\n",
    "# 3. ROC CURVE\n",
    "# -----------------------------------------\n",
    "\n",
    "fpr, tpr, _ = roc_curve(y_test_rr, y_proba_rf)\n",
    "\n",
    "plt.figure(figsize=(7,5))\n",
    "plt.plot(fpr, tpr, label=f\"ROC-AUC = {roc_auc_score(y_test_rr, y_proba_rf):.3f}\")\n",
    "plt.plot([0,1], [0,1], \"k--\")\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.title(\"ROC Curve — Random Forest\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "22d013a1-1900-4f42-81c1-a5069bfd90ff",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# -----------------------------------------\n",
    "# 4. PRECISION–RECALL CURVE\n",
    "# -----------------------------------------\n",
    "\n",
    "prec, rec, _ = precision_recall_curve(y_test_rr, y_proba_rf)\n",
    "\n",
    "plt.figure(figsize=(7,5))\n",
    "plt.plot(rec, prec, label=\"Precision-Recall curve\")\n",
    "plt.xlabel(\"Recall\")\n",
    "plt.ylabel(\"Precision\")\n",
    "plt.title(\"PR Curve — Random Forest\")\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9350a726-bfb3-47f0-ad60-0dd45d040eb9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# -----------------------------------------\n",
    "# 5. CALIBRATION CURVE\n",
    "# -----------------------------------------\n",
    "\n",
    "from sklearn.calibration import calibration_curve\n",
    "\n",
    "prob_true, prob_pred = calibration_curve(y_test_rr, y_proba_rf, n_bins=15)\n",
    "\n",
    "plt.figure(figsize=(7,5))\n",
    "plt.plot(prob_pred, prob_true, marker='o')\n",
    "plt.plot([0,1], [0,1], 'k--')\n",
    "plt.xlabel(\"Mean predicted probability\")\n",
    "plt.ylabel(\"True fraction of positives\")\n",
    "plt.title(\"Calibration Curve — Random Forest\")\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "46760ea5-dc49-4f40-842a-282d59f46905",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# -----------------------------------------\n",
    "# 6. FEATURE IMPORTANCE\n",
    "# -----------------------------------------\n",
    "\n",
    "importances = rf.feature_importances_\n",
    "indices = np.argsort(importances)[::-1]\n",
    "\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.bar(range(len(feature_cols_rr)), importances[indices])\n",
    "plt.xticks(range(len(feature_cols_rr)),\n",
    "           [feature_cols_rr[i] for i in indices],\n",
    "           rotation=45, ha='right')\n",
    "plt.title(\"Feature Importance — Random Forest\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "20e6af1d-5b34-4f3c-9757-e76f0c2d54b3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# -----------------------------------------\n",
    "# 7. SHAP summary (поглиблена інтерпретація)\n",
    "# -----------------------------------------\n",
    "\n",
    "shap.initjs()\n",
    "X_sample_rr = X_test_rr[:1500]\n",
    "\n",
    "explainer_rr = shap.TreeExplainer(rf)\n",
    "shap_values_rr = explainer_rr.shap_values(X_sample_rr)\n",
    "\n",
    "shap_vals_class1_rr = shap_values_rr[1] if isinstance(shap_values_rr, list) else shap_values_rr\n",
    "\n",
    "shap.summary_plot(\n",
    "    shap_vals_class1_rr,\n",
    "    X_sample_rr,\n",
    "    feature_names=feature_cols_rr\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "retailrocket_lab_4,5",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
