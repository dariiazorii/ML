{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5c63da89-de99-4d27-9692-17f2ec7fa4e5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 1. Завантаження даних\n",
    "\n",
    "delta_input_path_santander = \"/Volumes/workspace/default/santader_delta_dataset\"\n",
    "santander_df = spark.read.format(\"delta\").load(delta_input_path_santander)\n",
    "\n",
    "print(\"Rows:\", santander_df.count())\n",
    "display(santander_df.limit(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "033a8a18-c547-48f7-8469-945690b5704d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 2. Вибір числових ознак + кореляційний аналіз\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Всі числові колонки, крім ID_code і target\n",
    "numeric_cols = [c for c, t in santander_df.dtypes\n",
    "                if t in (\"double\", \"int\", \"float\", \"bigint\") and c not in (\"ID_code\", \"target\")]\n",
    "\n",
    "print(\"Кількість числових колонок:\", len(numeric_cols))\n",
    "\n",
    "# 10% вибірки для кореляцій, щоб не перевантажити памʼять\n",
    "sample_pd = santander_df.select(numeric_cols).sample(False, 0.1, seed=42).toPandas()\n",
    "\n",
    "corr = sample_pd.corr()\n",
    "print(\"Кореляційна матриця (фрагмент):\")\n",
    "display(corr.iloc[:10, :10]) \n",
    "\n",
    "# Сильно корельовані пари |r| > 0.8\n",
    "thr = 0.8\n",
    "high_corr = []\n",
    "for i, c1 in enumerate(corr.columns):\n",
    "    for j, c2 in enumerate(corr.columns):\n",
    "        if j > i and abs(corr.loc[c1, c2]) > thr:\n",
    "            high_corr.append((c1, c2, corr.loc[c1, c2]))\n",
    "\n",
    "print(\"Сильно корельовані пари (|r| > 0.8):\")\n",
    "for c1, c2, r in high_corr[:30]:\n",
    "    print(f\"{c1} - {c2}: r = {r:.3f}\")\n",
    "\n",
    "# Фільтраційний метод №1: видаляємо другу ознаку в кожній парі\n",
    "to_drop_corr = set()\n",
    "for c1, c2, r in high_corr:\n",
    "    # залишаємо першу, другу вважаємо надлишковою\n",
    "    to_drop_corr.add(c2)\n",
    "\n",
    "numeric_cols_corr = [c for c in numeric_cols if c not in to_drop_corr]\n",
    "print(\"Після видалення сильно корельованих ознак:\", len(numeric_cols_corr))\n",
    "\n",
    "santander_df = santander_df.select(\n",
    "    *[c for c in santander_df.columns if c not in to_drop_corr]  # дропаємо на рівні DataFrame\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "14b7d045-5303-4b33-afdf-7aa0c6c36457",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 3. Інженерія ознак (Feature Engineering, ≥5 нових ознак)\n",
    "\n",
    "from pyspark.sql.types import DoubleType\n",
    "from pyspark.sql.functions import udf, sqrt, pow\n",
    "\n",
    "# 3.1. Статистичні ознаки по рядку: mean, std, skewness, kurtosis\n",
    "fe_cols = numeric_cols_corr[:] \n",
    "\n",
    "def row_skew(values):\n",
    "    arr = np.array(values, dtype=float)\n",
    "    if len(arr) == 0 or np.std(arr) == 0:\n",
    "        return 0.0\n",
    "    return float(((arr - arr.mean())**3).mean() / (arr.std()**3 + 1e-9))\n",
    "\n",
    "def row_kurt(values):\n",
    "    arr = np.array(values, dtype=float)\n",
    "    if len(arr) == 0 or np.std(arr) == 0:\n",
    "        return 0.0\n",
    "    return float(((arr - arr.mean())**4).mean() / (arr.std()**4 + 1e-9))\n",
    "\n",
    "skew_udf = udf(row_skew, DoubleType())\n",
    "kurt_udf = udf(row_kurt, DoubleType())\n",
    "\n",
    "# Масив ознак для рядка\n",
    "santander_df = santander_df.withColumn(\"features_array\", F.array(*[col(c) for c in fe_cols]))\n",
    "\n",
    "santander_df = (\n",
    "    santander_df\n",
    "    .withColumn(\n",
    "        \"row_mean\",\n",
    "        F.expr(\"aggregate(features_array, 0D, (acc, x) -> acc + x) / size(features_array)\")\n",
    "    )\n",
    "    .withColumn(\n",
    "        \"row_std\",\n",
    "        F.expr(\n",
    "            \"sqrt(aggregate(transform(features_array, x -> pow(x - row_mean, 2)), \"\n",
    "            \"0D, (acc, x) -> acc + x) / size(features_array))\"\n",
    "        )\n",
    "    )\n",
    "    .withColumn(\"row_skew\", skew_udf(col(\"features_array\")))\n",
    "    .withColumn(\"row_kurt\", kurt_udf(col(\"features_array\")))\n",
    ")\n",
    "\n",
    "# 3.2. Interaction features (x1 * x2, x3 * x4)\n",
    "interaction_pairs = list(zip(fe_cols[:6:2], fe_cols[1:6:2]))\n",
    "for f1, f2 in interaction_pairs:\n",
    "    new_name = f\"{f1}_x_{f2}\"\n",
    "    santander_df = santander_df.withColumn(new_name, col(f1) * col(f2))\n",
    "\n",
    "# 3.3. Distance-based feature (відстань у підпросторі перших 4 змінних)\n",
    "if len(fe_cols) >= 4:\n",
    "    santander_df = santander_df.withColumn(\n",
    "        \"distance_feat\",\n",
    "        sqrt(\n",
    "            pow(col(fe_cols[0]) - col(fe_cols[1]), 2) +\n",
    "            pow(col(fe_cols[2]) - col(fe_cols[3]), 2)\n",
    "        )\n",
    "    )\n",
    "\n",
    "# Прибираємо допоміжний масив\n",
    "santander_df = santander_df.drop(\"features_array\")\n",
    "\n",
    "# Оновлюємо список числових колонок (додаємо нові фічі)\n",
    "new_fe_cols = [\"row_mean\", \"row_std\", \"row_skew\", \"row_kurt\", \"distance_feat\"] + \\\n",
    "              [f\"{f1}_x_{f2}\" for f1, f2 in interaction_pairs]\n",
    "\n",
    "numeric_cols_fe = numeric_cols_corr + [c for c in new_fe_cols if c in santander_df.columns]\n",
    "\n",
    "print(\"Кількість числових ознак після FE:\", len(numeric_cols_fe))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0cd452f5-43f9-415d-a533-8ec4b5f332fc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 4. Обробка викидів (IQR) – для кількох колонок\n",
    "# перші 10 числових колонок\n",
    "iqr_cols = numeric_cols_corr[:10]\n",
    "\n",
    "iqr_bounds = {}\n",
    "for c in iqr_cols:\n",
    "    q1, q3 = santander_df.approxQuantile(c, [0.25, 0.75], 0.01)\n",
    "    iqr = q3 - q1\n",
    "    low, high = q1 - 1.5 * iqr, q3 + 1.5 * iqr\n",
    "    iqr_bounds[c] = (low, high)\n",
    "\n",
    "print(\"IQR межі (фрагмент):\")\n",
    "for c, (low, high) in list(iqr_bounds.items())[:5]:\n",
    "    print(f\"{c}: [{low:.4f}, {high:.4f}]\")\n",
    "\n",
    "# Фільтруємо викиди тільки по цих кількох колонках\n",
    "for c, (low, high) in iqr_bounds.items():\n",
    "    santander_df = santander_df.filter((col(c) >= low) & (col(c) <= high))\n",
    "\n",
    "print(\"Рядків після фільтрації IQR:\", santander_df.count())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "502dc545-143c-4ae8-ab50-695aa2ff0740",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 5. Вибір ознак (Feature Selection: Variance + L1)\n",
    "\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "# 5.1. Фільтраційний метод №2: Variance + top-K\n",
    "var_row = santander_df.select(\n",
    "    [F.variance(col(c)).alias(c) for c in numeric_cols_fe]\n",
    ").collect()[0].asDict()\n",
    "\n",
    "var_items = [(c, float(var_row.get(c) or 0.0)) for c in numeric_cols_fe]\n",
    "var_items_sorted = sorted(var_items, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "TOP_K = 30 \n",
    "topk_feats = [c for c, _ in var_items_sorted[:TOP_K]]\n",
    "\n",
    "print(\"Топ-30 ознак за дисперсією:\")\n",
    "for c, v in var_items_sorted[:10]:\n",
    "    print(f\"{c}: var = {v:.6f}\")\n",
    "\n",
    "# 5.2. стандартизація (z-score) тільки для top-k ознак\n",
    "stats = santander_df.select(\n",
    "    *[F.mean(col(c)).alias(f\"{c}_mean\") for c in topk_feats],\n",
    "    *[F.stddev(col(c)).alias(f\"{c}_std\") for c in topk_feats]\n",
    ").collect()[0].asDict()\n",
    "\n",
    "scaled_cols = []\n",
    "for c in topk_feats:\n",
    "    mean_val = stats.get(f\"{c}_mean\") or 0.0\n",
    "    std_val = stats.get(f\"{c}_std\") or 1.0\n",
    "    if std_val == 0:\n",
    "        std_val = 1.0\n",
    "    scaled_name = f\"{c}_scaled\"\n",
    "    santander_df = santander_df.withColumn(scaled_name, (col(c) - mean_val) / std_val)\n",
    "    scaled_cols.append(scaled_name)\n",
    "\n",
    "# вектор scaled_features\n",
    "assembler_scaled = VectorAssembler(inputCols=scaled_cols, outputCol=\"scaled_features\")\n",
    "santander_df = assembler_scaled.transform(santander_df)\n",
    "\n",
    "print(f\"Розмірність scaled_features: {len(scaled_cols)}\")\n",
    "\n",
    "# 5.3. метод: L1 Logistic Regression\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "santander_l1 = santander_df.select(\"target\", \"scaled_features\").where(col(\"target\").isNotNull())\n",
    "\n",
    "lr = LogisticRegression(\n",
    "    featuresCol=\"scaled_features\",\n",
    "    labelCol=\"target\",\n",
    "    regParam=0.1,\n",
    "    elasticNetParam=1.0  # L1\n",
    ")\n",
    "\n",
    "lr_model = lr.fit(santander_l1)\n",
    "\n",
    "coef = lr_model.coefficients.toArray()\n",
    "feature_importance_l1 = sorted(\n",
    "    [(f, float(w)) for f, w in zip(topk_feats, coef)],\n",
    "    key=lambda x: abs(x[1]),\n",
    "    reverse=True\n",
    ")[:10]\n",
    "\n",
    "print(\"Топ-10 важливих ознак за L1 Logistic Regression:\")\n",
    "for name, weight in feature_importance_l1:\n",
    "    print(f\"{name}: {weight:.6f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a21835c1-dd91-4385-9ac3-7574ae473c38",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 6. PCA\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "# топ-20 фіч після Variance Selection\n",
    "top20 = ['var_27', 'var_17', 'var_84', 'var_178', 'var_176',\n",
    "         'var_93', 'var_38', 'var_68', 'var_85', 'var_83',\n",
    "         'var_73', 'var_58', 'var_182', 'var_137', 'var_160',\n",
    "         'var_64', 'var_39', 'var_192', 'var_163', 'var_104']\n",
    "\n",
    "# Вибірка\n",
    "sample_pd = santander_df.select(top20).sample(fraction=0.03, seed=42).toPandas()\n",
    "\n",
    "print(\"Sample shape:\", sample_pd.shape)\n",
    "\n",
    "# Стандартизація\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler_np = StandardScaler()\n",
    "X_scaled = scaler_np.fit_transform(sample_pd.values)\n",
    "\n",
    "# PCA\n",
    "from sklearn.decomposition import PCA\n",
    "pca_np = PCA(n_components=5)\n",
    "X_pca = pca_np.fit_transform(X_scaled)\n",
    "\n",
    "print(\"\\nПояснена дисперсія:\", pca_np.explained_variance_ratio_)\n",
    "print(\"Сумарна пояснена:\", pca_np.explained_variance_ratio_.sum())\n",
    "\n",
    "\n",
    "pca_pdf = pd.DataFrame(X_pca, columns=[f\"pca_{i+1}\" for i in range(5)])\n",
    "pca_spark_df = spark.createDataFrame(pca_pdf)\n",
    "\n",
    "from pyspark.sql import functions as F\n",
    "santander_sample = santander_df.sample(fraction=0.03, seed=42) \\\n",
    "                               .withColumn(\"row_idx\", F.monotonically_increasing_id())\n",
    "\n",
    "pca_spark_df = pca_spark_df.withColumn(\"row_idx\", F.monotonically_increasing_id())\n",
    "\n",
    "santander_pca_final = santander_sample.join(pca_spark_df, on=\"row_idx\").drop(\"row_idx\")\n",
    "\n",
    "display(santander_pca_final.limit(5))\n",
    "\n",
    "pca_sample_pd = pd.DataFrame(X_pca, columns=[f\"pca_{i+1}\" for i in range(5)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "936f4502-c50f-47f6-a6dd-513c4d6d4f6e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "%pip install umap-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "deecab28-6a00-4a4b-a709-3c601442f238",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import umap\n",
    "\n",
    "umap_model = umap.UMAP(n_components=2, random_state=42)\n",
    "X_umap = umap_model.fit_transform(X_pca)\n",
    "\n",
    "print(\"UMAP виконано успішно. Розмірність:\", X_umap.shape)\n",
    "\n",
    "pca_sample_pd[\"umap_1\"] = X_umap[:, 0]\n",
    "pca_sample_pd[\"umap_2\"] = X_umap[:, 1]\n",
    "\n",
    "display(pca_sample_pd[[\"umap_1\", \"umap_2\"]].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f4c2b158-5776-4360-9552-5099eb7ce5ea",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Перетворюємо UMAP-таблицю назад у Spark\n",
    "umap_pdf = pca_sample_pd[[\"umap_1\", \"umap_2\"]]\n",
    "umap_spark_df = spark.createDataFrame(umap_pdf)\n",
    "\n",
    "umap_spark_df = umap_spark_df.withColumn(\"row_idx\", F.monotonically_increasing_id())\n",
    "santander_pca_final = santander_pca_final.withColumn(\"row_idx\", F.monotonically_increasing_id())\n",
    "\n",
    "# PCA + UMAP\n",
    "santander_final_df = santander_pca_final.join(umap_spark_df, on=\"row_idx\").drop(\"row_idx\")\n",
    "\n",
    "display(santander_final_df.limit(5))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e585ec22-6e05-4db7-9bac-aa043075e9f2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "output_path_santander_final = \"/Volumes/workspace/default/santader_delta_3\"\n",
    "\n",
    "santander_final_df.write.format(\"delta\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .option(\"overwriteSchema\", \"true\") \\\n",
    "    .save(output_path_santander_final)\n",
    "\n",
    "print(\"Фінальний датасет Santander з PCA + UMAP успішно збережений!\")\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "santader_lab_3",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
