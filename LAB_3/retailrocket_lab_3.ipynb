{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1c1c14b8-96ba-4d18-926d-7692cdbdc85a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "delta_input_path_rr = \"/Volumes/workspace/default/retail_delta_2\"\n",
    "retailrocket_df = spark.read.format(\"delta\").load(delta_input_path_rr)\n",
    "\n",
    "display(retailrocket_df.limit(5))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3571afe3-2aa3-489d-a89c-d018543eceb7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# 2. Кореляційний аналіз та видалення сильно корельованих ознак\n",
    "\n",
    "import numpy as np\n",
    "from pyspark.sql.functions import col, when, avg, lit\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "numeric_cols_rr = [\n",
    "    \"event_indexed_mean\", \"event_index_mean\", \"itemid_index_mean\",\n",
    "    \"time_to_purchase_mean\", \"year_mean\", \"month_mean\", \"day_mean\",\n",
    "    \"weekday_mean\", \"weekofyear_mean\", \"events_per_session_mean\",\n",
    "    \"add_to_cart_ratio_mean\"\n",
    "]\n",
    "\n",
    "# Кореляційна матриця (Pandas)\n",
    "df_numeric_rr_pd = retailrocket_df.select(numeric_cols_rr).toPandas()\n",
    "corr_matrix_rr = df_numeric_rr_pd.corr()\n",
    "print(\"Кореляційна матриця числових ознак RetailRocket:\")\n",
    "print(corr_matrix_rr)\n",
    "\n",
    "# сильно корельовані пари |r| > 0.8\n",
    "threshold = 0.8\n",
    "high_corr_rr = np.where(np.abs(corr_matrix_rr) > threshold)\n",
    "high_corr_pairs_rr = [(numeric_cols_rr[i], numeric_cols_rr[j], corr_matrix_rr.iloc[i,j])\n",
    "                      for i,j in zip(*high_corr_rr) if i < j]\n",
    "\n",
    "print(\"\\nСильно корельовані пари (|r| > 0.8):\")\n",
    "for pair in high_corr_pairs_rr:\n",
    "    print(pair)\n",
    "\n",
    "# Видаляємо сильно корельовані ознаки\n",
    "for col1, col2, corr_val in high_corr_pairs_rr:\n",
    "    print(f\"Видаляємо {col2} через сильну кореляцію з {col1} (r={corr_val:.2f})\")\n",
    "    if col2 in retailrocket_df.columns:\n",
    "        retailrocket_df = retailrocket_df.drop(col2)\n",
    "        if col2 in numeric_cols_rr:\n",
    "            numeric_cols_rr.remove(col2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "123d7b61-13b0-4172-a670-88251110d9cf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 3. Створення нових ознак\n",
    "\n",
    "w = Window.partitionBy() \n",
    "\n",
    "retailrocket_df = retailrocket_df.withColumn(\n",
    "    \"event_indexed_sq\", col(\"event_indexed_mean\")**2\n",
    ").withColumn(\n",
    "    \"event_item_interaction\", col(\"event_indexed_mean\") * col(\"itemid_index_mean\")\n",
    ").withColumn(\n",
    "    \"month_ratio\", col(\"month_mean\") / lit(12)\n",
    ").withColumn(\n",
    "    \"high_event_indexed\", when(col(\"event_indexed_mean\") > 1, 1).otherwise(0)\n",
    ").withColumn(\n",
    "    \"event_indexed_diff_avg\", col(\"event_indexed_mean\") - avg(\"event_indexed_mean\").over(w)\n",
    ")\n",
    "\n",
    "numeric_cols_rr += [\n",
    "    \"event_indexed_sq\", \"event_item_interaction\",\n",
    "    \"month_ratio\", \"high_event_indexed\", \"event_indexed_diff_avg\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "00d6d55e-62fc-4c1a-b16d-5d1182c3d44e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 4. Масштабування ознак\n",
    "\n",
    "from pyspark.ml.feature import VectorAssembler, StandardScaler\n",
    "\n",
    "cols_to_drop = [\"numeric_vector_scaled\", \"numeric_scaled_features\"]\n",
    "for c in cols_to_drop:\n",
    "    if c in retailrocket_df.columns:\n",
    "        retailrocket_df = retailrocket_df.drop(c)\n",
    "\n",
    "assembler = VectorAssembler(inputCols=numeric_cols_rr, outputCol=\"numeric_vector_scaled\")\n",
    "retailrocket_df = assembler.transform(retailrocket_df)\n",
    "\n",
    "scaler = StandardScaler(\n",
    "    inputCol=\"numeric_vector_scaled\",\n",
    "    outputCol=\"numeric_scaled_features\",\n",
    "    withMean=True,\n",
    "    withStd=True\n",
    ")\n",
    "scaler_model = scaler.fit(retailrocket_df)\n",
    "retailrocket_df = scaler_model.transform(retailrocket_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7b55bcef-6f79-4bd0-9594-3b1bd289ecbe",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 5a. PCA loadings\n",
    "from pyspark.ml.feature import PCA\n",
    "\n",
    "pca = PCA(k=5, inputCol=\"numeric_scaled_features\", outputCol=\"pca_features\")\n",
    "pca_model = pca.fit(retailrocket_df)\n",
    "retailrocket_df = pca_model.transform(retailrocket_df)\n",
    "\n",
    "explained_var = pca_model.explainedVariance.toArray().sum()\n",
    "print(f\"Сумарна пояснена дисперсія PCA (5 компонент): {explained_var:.4f}\")\n",
    "\n",
    "pca_loadings = np.array(pca_model.pc.toArray())\n",
    "feature_importance_pca = np.sum(np.abs(pca_loadings), axis=1)\n",
    "\n",
    "top_features_pca = sorted(\n",
    "    zip(numeric_cols_rr, feature_importance_pca),\n",
    "    key=lambda x: x[1],\n",
    "    reverse=True\n",
    ")[:10]\n",
    "\n",
    "print(\"Топ-10 ознак за важливістю PCA loadings:\")\n",
    "for name, weight in top_features_pca:\n",
    "    print(f\"{name}: {weight:.6f}\")\n",
    "\n",
    "# ---------------------------------------------\n",
    "# 5b. Variance Threshold\n",
    "from pyspark.ml.stat import Summarizer\n",
    "\n",
    "assembler_var = VectorAssembler(inputCols=numeric_cols_rr, outputCol=\"features_vec_var\")\n",
    "df_vec_var = assembler_var.transform(retailrocket_df)\n",
    "\n",
    "summarizer = Summarizer.metrics(\"variance\")\n",
    "variance_df = df_vec_var.select(Summarizer.variance(df_vec_var[\"features_vec_var\"]).alias(\"variances\")).collect()\n",
    "variances = variance_df[0][\"variances\"]\n",
    "\n",
    "variance_threshold = 0.01\n",
    "selected_features_var = [col_name for col_name, var in zip(numeric_cols_rr, variances) if var > variance_threshold]\n",
    "\n",
    "print(\"\\nОзнаки, відібрані за порогом дисперсії (>0.01):\")\n",
    "print(selected_features_var)\n",
    "\n",
    "# Порівняння з PCA\n",
    "pca_top_names = [name for name, _ in top_features_pca]\n",
    "print(\"\\nПорівняння ознак: PCA top-10 vs Variance Threshold\")\n",
    "print(\"PCA:\", pca_top_names)\n",
    "print(\"Variance Threshold:\", selected_features_var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8a4e9e6b-cf0b-4a57-b55f-57d892bf1b65",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "%pip install umap-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ba51631d-3e85-419a-88e1-020cd4989a68",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "import umap\n",
    "import pandas as pd\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "features_pd = retailrocket_df.select(\"numeric_scaled_features\").toPandas()\n",
    "features_array = np.vstack(features_pd[\"numeric_scaled_features\"].values)\n",
    "\n",
    "umap_model = umap.UMAP(n_components=2, random_state=42)\n",
    "umap_features = umap_model.fit_transform(features_array)\n",
    "\n",
    "umap_df = pd.DataFrame(umap_features, columns=[\"umap_1\", \"umap_2\"])\n",
    "\n",
    "# Конвертуємо у Spark DataFrame\n",
    "umap_spark_df = spark.createDataFrame(umap_df)\n",
    "retailrocket_df = retailrocket_df.withColumn(\"row_idx\", F.monotonically_increasing_id())\n",
    "umap_spark_df = umap_spark_df.withColumn(\"row_idx\", F.monotonically_increasing_id())\n",
    "\n",
    "retailrocket_df = retailrocket_df.join(umap_spark_df, on=\"row_idx\").drop(\"row_idx\")\n",
    "\n",
    "# scatter plot\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "umap_pd = retailrocket_df.select(\"umap_1\", \"umap_2\").toPandas()\n",
    "plt.figure(figsize=(6,5))\n",
    "plt.scatter(umap_pd[\"umap_1\"], umap_pd[\"umap_2\"], s=5, alpha=0.6)\n",
    "plt.title(\"UMAP 2D projection of RetailRocket features\")\n",
    "plt.xlabel(\"UMAP 1\")\n",
    "plt.ylabel(\"UMAP 2\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3762f972-9135-4332-8157-3766816871f7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "output_path_rr_final = \"/Volumes/workspace/default/retail_delta_3\"\n",
    "retailrocket_df.write.format(\"delta\").mode(\"overwrite\").save(output_path_rr_final)\n",
    "\n",
    "print(\"Фінальний набір ознак RetailRocket успішно збережено в Delta Lake!\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "retailrocket_lab_3",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
