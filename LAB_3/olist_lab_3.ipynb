{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9b1bc2a9-ed01-4cad-b7ee-5fe81a8d19ca",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 1. Завантаження даних\n",
    "\n",
    "delta_input_path = \"/Volumes/workspace/default/olist_delta_2\"\n",
    "olist_df = spark.read.format(\"delta\").load(delta_input_path)\n",
    "\n",
    "print(\"Rows:\", olist_df.count())\n",
    "display(olist_df.limit(5))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "545663f7-de40-40a9-81f1-276aa0ad6456",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 2. Вибір числових ознак\n",
    "\n",
    "from pyspark.sql.functions import udf, col\n",
    "from pyspark.sql.types import ArrayType, DoubleType\n",
    "\n",
    "def vector_to_list_safe(v):\n",
    "    if v is None:\n",
    "        return None\n",
    "    try:\n",
    "        return [float(x) for x in v.toArray()]\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "vector_to_list_udf = udf(vector_to_list_safe, ArrayType(DoubleType()))\n",
    "\n",
    "if \"numeric_scaled\" in olist_df.columns:\n",
    "    olist_df = olist_df.withColumn(\"numeric_array\", vector_to_list_udf(col(\"numeric_scaled\")))\n",
    "    num_features = 6\n",
    "    numeric_cols = [f\"num_{i+1}\" for i in range(num_features)]\n",
    "    for i, cname in enumerate(numeric_cols):\n",
    "        olist_df = olist_df.withColumn(cname, col(\"numeric_array\")[i])\n",
    "else:\n",
    "    numeric_cols = [c for c, t in olist_df.dtypes if t in (\"double\", \"float\", \"int\", \"bigint\")][:30]\n",
    "\n",
    "print(\"Numeric cols:\", numeric_cols[:30])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e79e2e8b-3ae6-4ae6-a332-1020e1134f71",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Беремо підвибірку ~10% для кореляції, щоб не грузити пам'ять\n",
    "sample_pd = olist_df.select(numeric_cols).sample(False, 0.1, seed=42).toPandas()\n",
    "corr = sample_pd.corr()\n",
    "display(corr)\n",
    "\n",
    "# Знайдемо сильно корельовані пари |r| > 0.8\n",
    "thr = 0.8\n",
    "high_corr = [\n",
    "    (c1, c2, corr.loc[c1, c2])\n",
    "    for i, c1 in enumerate(corr.columns)\n",
    "    for j, c2 in enumerate(corr.columns)\n",
    "    if j > i and abs(corr.loc[c1, c2]) > thr\n",
    "]\n",
    "print(\"High correlations:\", high_corr)\n",
    "\n",
    "# ВІДБІР ОЗНАК: видаляємо другу ознаку з пари (фільтраційний метод 1 — correlation)\n",
    "for c1, c2, r in high_corr:\n",
    "    print(f\"Drop {c2} because of high correlation with {c1} (r={r:.2f})\")\n",
    "    if c2 in olist_df.columns:\n",
    "        olist_df = olist_df.drop(c2)\n",
    "    if c2 in numeric_cols:\n",
    "        numeric_cols.remove(c2)\n",
    "\n",
    "print(\"Numeric cols after correlation-based selection:\", len(numeric_cols))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d8deda09-7c29-43aa-a96f-a04f5277b130",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 4. Інженерія нових ознак (мінімум 5)\n",
    "\n",
    "from pyspark.sql.functions import when, lit, mean as _mean\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "# 1) Ознаки, пов’язані з часом доставки\n",
    "if \"delivery_time_days_calc\" in olist_df.columns:\n",
    "    avg_delivery = olist_df.select(_mean(col(\"delivery_time_days_calc\"))).first()[0] or 0.0\n",
    "    olist_df = olist_df.withColumn(\n",
    "        \"delivery_diff_avg\",\n",
    "        col(\"delivery_time_days_calc\") - lit(avg_delivery)\n",
    "    )\n",
    "    olist_df = olist_df.withColumn(\n",
    "        \"fast_delivery\",\n",
    "        when(col(\"delivery_time_days_calc\") < 3, 1).otherwise(0)\n",
    "    )\n",
    "else:\n",
    "    olist_df = olist_df.withColumn(\"delivery_diff_avg\", lit(0.0)) \\\n",
    "                       .withColumn(\"fast_delivery\", lit(0))\n",
    "\n",
    "# 2) Квадрат першої числової ознаки\n",
    "olist_df = olist_df.withColumn(\"num_1_sq\", (col(\"num_1\") ** 2).cast(\"double\"))\n",
    "\n",
    "# 3) Взаємодія між двома фічами\n",
    "olist_df = olist_df.withColumn(\n",
    "    \"num2_num3_interaction\",\n",
    "    (col(\"num_2\") * col(\"num_3\")).cast(\"double\")\n",
    ")\n",
    "\n",
    "# 4) Середнє двох перших числових ознак\n",
    "olist_df = olist_df.withColumn(\n",
    "    \"num_mean_12\",\n",
    "    ((col(\"num_1\") + col(\"num_2\")) / 2).cast(\"double\")\n",
    ")\n",
    "\n",
    "# 5) Логарифм від num_1 (робимо log1p(abs(x)))\n",
    "olist_df = olist_df.withColumn(\n",
    "    \"log_num_1\",\n",
    "    F.log1p(F.abs(col(\"num_1\")))\n",
    ")\n",
    "\n",
    "# Додаємо нові фічі до списку числових\n",
    "for f in [\"delivery_diff_avg\", \"fast_delivery\",\n",
    "          \"num_1_sq\", \"num2_num3_interaction\",\n",
    "          \"num_mean_12\", \"log_num_1\"]:\n",
    "    if f in olist_df.columns and f not in numeric_cols:\n",
    "        numeric_cols.append(f)\n",
    "\n",
    "print(\"Нові ознаки додано. Всього числових:\", len(numeric_cols))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "defa2bc0-a723-4e70-a95a-428368569b2f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 5. Масштабування + Відбір ознак (Метод 2 — Variance Threshold)\n",
    "\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "# Обчислюємо дисперсію для кожної фічі\n",
    "var_row = olist_df.select([F.variance(col(c)).alias(c) for c in numeric_cols]).collect()[0].asDict()\n",
    "var_items = [(c, float(var_row.get(c) or 0.0)) for c in numeric_cols]\n",
    "\n",
    "# Фільтраційний метод 2: відбір за порогом дисперсії\n",
    "variance_threshold = 0.01\n",
    "selected_by_variance = [c for c, v in var_items if v > variance_threshold]\n",
    "\n",
    "print(\"Ознаки, відібрані за порогом дисперсії (> 0.01):\")\n",
    "print(selected_by_variance)\n",
    "\n",
    "# Для масштабування фічs з найвищою дисперсією\n",
    "var_items_sorted = sorted(var_items, key=lambda x: x[1], reverse=True)\n",
    "topk = [c for c, _ in var_items_sorted[:50]]\n",
    "\n",
    "# mean і std тільки для top-k\n",
    "stats = olist_df.select(\n",
    "    *[F.mean(col(c)).alias(f\"{c}_mean\") for c in topk],\n",
    "    *[F.stddev(col(c)).alias(f\"{c}_std\") for c in topk]\n",
    ").collect()[0].asDict()\n",
    "\n",
    "# Масштабування (z-score)\n",
    "for c in topk:\n",
    "    mean_val = stats.get(f\"{c}_mean\") or 0.0\n",
    "    std_val = stats.get(f\"{c}_std\") or 1.0\n",
    "    if std_val == 0:\n",
    "        std_val = 1.0\n",
    "    olist_df = olist_df.withColumn(f\"{c}_scaled\", (col(c) - mean_val) / std_val)\n",
    "\n",
    "scaled_cols = [f\"{c}_scaled\" for c in topk]\n",
    "\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=scaled_cols,\n",
    "    outputCol=\"numeric_scaled_features\",\n",
    "    handleInvalid=\"skip\"\n",
    ")\n",
    "olist_df = assembler.transform(olist_df)\n",
    "\n",
    "print(f\"Масштабування виконано для {len(scaled_cols)} фіч.\")\n",
    "display(olist_df.select(\"numeric_scaled_features\").limit(3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ab43c911-90f5-415a-b504-b36ee040e728",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# вибірка для аналізу\n",
    "pca_sample_pd = olist_df.select(scaled_cols).sample(False, 0.02, seed=42).toPandas()\n",
    "\n",
    "pca_input_cols = scaled_cols[:20]\n",
    "X_pca = pca_sample_pd[pca_input_cols].values\n",
    "\n",
    "pca = PCA(n_components=3, random_state=42)\n",
    "X_pca_trans = pca.fit_transform(X_pca)\n",
    "\n",
    "print(\"PCA (sklearn) виконано успішно.\")\n",
    "print(\"Пояснена дисперсія по компонентах:\", pca.explained_variance_ratio_)\n",
    "print(\"Сумарна пояснена дисперсія:\", pca.explained_variance_ratio_.sum())\n",
    "\n",
    "# Feature selection через PCA loadings\n",
    "loadings = np.abs(pca.components_).sum(axis=0)  # сумарний внесок кожної фічі\n",
    "pca_feature_importance = sorted(\n",
    "    zip(pca_input_cols, loadings),\n",
    "    key=lambda x: x[1],\n",
    "    reverse=True\n",
    ")[:10]\n",
    "\n",
    "print(\"Топ-10 найважливіших ознак за PCA loadings:\")\n",
    "for name, score in pca_feature_importance:\n",
    "    print(f\"{name}: {score:.6f}\")\n",
    "\n",
    "pca_sample_pd[[\"pca_1\", \"pca_2\", \"pca_3\"]] = X_pca_trans\n",
    "display(pca_sample_pd[[\"pca_1\", \"pca_2\", \"pca_3\"]].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fb9d0785-f6bf-4f66-ba1c-25004966bbac",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install umap-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "96eaa956-afe3-4b10-bfe1-7b504c51fec3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 7. Додаткове зменшення розмірності: UMAP \n",
    "\n",
    "import umap\n",
    "\n",
    "umap_model = umap.UMAP(n_components=2, random_state=42)\n",
    "X_umap = umap_model.fit_transform(X_pca)\n",
    "\n",
    "print(\"UMAP виконано успішно. Розмірність:\", X_umap.shape)\n",
    "\n",
    "pca_sample_pd[\"umap_1\"] = X_umap[:, 0]\n",
    "pca_sample_pd[\"umap_2\"] = X_umap[:, 1]\n",
    "\n",
    "display(pca_sample_pd[[\"umap_1\", \"umap_2\"]].head())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "45835ca2-6609-4392-9321-60c65bf3dd83",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 8. ВІЗУАЛІЗАЦІЯ UMAP\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "print(\"Візуалізація UMAP...\")\n",
    "\n",
    "# Налаштування стилю\n",
    "sns.set_style(\"whitegrid\")\n",
    "\n",
    "# Створення фігури та осей\n",
    "plt.figure(figsize=(10, 8))\n",
    "\n",
    "# Побудова точкової діаграми\n",
    "sns.scatterplot(\n",
    "    x='umap_1', \n",
    "    y='umap_2', \n",
    "    data=pca_sample_pd, \n",
    "    s=10, # Розмір точок\n",
    "    alpha=0.6, # Прозорість\n",
    "    \n",
    ")\n",
    "\n",
    "plt.title('UMAP Проєкція даних (2D)', fontsize=16)\n",
    "plt.xlabel('UMAP Dimension 1')\n",
    "plt.ylabel('UMAP Dimension 2')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8568e220-185c-4678-b066-c45d748b64dd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 8. Збереження результату\n",
    "\n",
    "output_path = \"/Volumes/workspace/default/olist_delta_3\"\n",
    "\n",
    "olist_df.write.format(\"delta\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .option(\"overwriteSchema\", \"true\") \\\n",
    "    .save(output_path)\n",
    "\n",
    "print(\"Результат збережено у:\", output_path)\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "olist_lab_3",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
